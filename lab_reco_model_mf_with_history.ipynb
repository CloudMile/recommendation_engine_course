{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, json, time\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, itertools, shutil\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "from utils.utils import *\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "randomSeed = 88\n",
    "np.random.seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, movies, uidEnc, midEnc, nUsers, nMovies, midMap, tr, te, trRatingMat, teRatingMat = prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies profile\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以leave one out方式產生 train data, test data\n",
    "1. 每一筆資料有兩部分: [user query] + [item id]\n",
    "2. 每一筆user query 包含所有user movie history, 除了當前的rating movie (candidate movie)\n",
    "3. test data的user query來自於train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, train_hist=None, is_train=True):\n",
    "    queue = []\n",
    "    for u, df in data.groupby(\"userId\"):\n",
    "        df = df.sort_values(\"rating\", ascending=False)\n",
    "        if not is_train:\n",
    "            user_movies_hist = train_hist.query(\"userId == {}\".format(u)).movieId\n",
    "        for i, (_, r) in enumerate(df.iterrows()):\n",
    "            if is_train:\n",
    "                queue.append([int(r.userId), df.movieId[:i].tolist() + df.movieId[i + 1:].tolist(), int(r.movieId), r.rating])\n",
    "            else:\n",
    "                # all_hist = set(user_movies_hist.tolist() + df.movieId[:i].tolist())\n",
    "                all_hist = set(user_movies_hist.tolist())\n",
    "                queue.append([int(r.userId), list(all_hist - set([int(r.movieId)])), int(r.movieId), r.rating])\n",
    "    return pd.DataFrame(queue, columns=[\"user_id\", \"query_movie_ids\", \"candidate_movie_id\", \"rating\"])\n",
    "\n",
    "trProcessed = preprocess(tr)\n",
    "teProcessed = preprocess(te, tr, is_train=False)\n",
    "trProcessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teProcessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Function\n",
    "1. 由於tensorflow placeholder不支援變動長度的columns, 需透過padding zero(補零)帶入\n",
    "2. 每個變動長度的column, 需要再給lens描述每一筆資料的長度, ex: query_movie_ids, query_movie_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataFn(data, n_batch=128, shuffle=False):\n",
    "    pad = tf.keras.preprocessing.sequence.pad_sequences\n",
    "    def fn():\n",
    "        dataInner = data.copy()\n",
    "        indices = utils.get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n",
    "        for ind in indices:\n",
    "            yield do_multi(dataInner.iloc[ind], [\"query_movie_ids\"])\n",
    "    return fn\n",
    "\n",
    "for i, e in enumerate(dataFn(trProcessed, n_batch=3, shuffle=True)(), 1):\n",
    "    break\n",
    "pd.DataFrame(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Tensorflow Model Based Matrix Factorization With History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMFWithHistory(object):\n",
    "    def __init__(self, n_items, dim=32, learning_rate=0.01, reg=0.05, modelDir=\"./model/model_mf_with_history\"):\n",
    "        self.n_items = n_items\n",
    "        self.dim = dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg = reg\n",
    "        self.modelDir = modelDir\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            # inputs parts\n",
    "            graph_inputs(self)\n",
    "            # embedding parts\n",
    "            graph_embedding(self)\n",
    "            # computation parts\n",
    "            graph_computation(self)\n",
    "            # loss parts\n",
    "            graph_loss(self)\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            self.graph = graph\n",
    "        \n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        return fit(self, sess, trainGen, testGen, reset=reset, nEpoch=nEpoch)\n",
    "    \n",
    "    def predict(self, sess, users: list):\n",
    "        return predict(self, sess, users)\n",
    "    \n",
    "    def resetModel(self, modelDir):\n",
    "        \"\"\"刪除model dir\"\"\"\n",
    "        shutil.rmtree(path=modelDir, ignore_errors=True)\n",
    "        os.makedirs(modelDir)\n",
    "        \n",
    "    def ckpt(self, sess, modelDir):\n",
    "        \"\"\"load latest saved model\"\"\"\n",
    "        latestCkpt = tf.train.latest_checkpoint(modelDir)\n",
    "        if latestCkpt:\n",
    "            self.saver.restore(sess, latestCkpt)\n",
    "        return latestCkpt\n",
    "    \n",
    "    def feed_dict(self, data, mode=\"train\"):\n",
    "        \"\"\"處理資料進placeholder的邏輯\"\"\"\n",
    "        ret = {\n",
    "            self.query_movie_ids: data[\"query_movie_ids\"],\n",
    "            self.query_movie_ids_len: data[\"query_movie_ids_len\"],\n",
    "            self.candidate_movie_id: data[\"candidate_movie_id\"]\n",
    "        }\n",
    "        ret[self.isTrain] = False\n",
    "        if mode != \"infer\":\n",
    "            ret[self.rating] = data[\"rating\"]\n",
    "            if mode == \"train\":\n",
    "                ret[self.isTrain] = True\n",
    "            elif mode == \"eval\":\n",
    "                pass\n",
    "        return ret\n",
    "    \n",
    "    def epochLoss(self, sess, dataGen, tpe=\"rmse\"):\n",
    "        totLoss, totCnt = 0, 0\n",
    "        # loss_map = {\"rmse\": self.rmse_loss, \"mae\": self.mae_loss, \"target_loss\": self.loss}\n",
    "        for data in dataGen():\n",
    "            lossTensor = self.rmse_loss if tpe == \"rmse\" else self.mae_loss\n",
    "            loss = sess.run(lossTensor, feed_dict=self.feed_dict(data, mode=\"eval\"))\n",
    "            totLoss += (loss ** 2 if tpe == \"rmse\" else loss) * len(data[\"query_movie_ids\"])\n",
    "            totCnt += len(data[\"query_movie_ids\"])\n",
    "        return np.sqrt(totLoss / totCnt) if tpe == \"rmse\" else totLoss / totCnt\n",
    "    \n",
    "    def evaluateRMSE(self, sess, dataGen):\n",
    "        \"\"\"計算root mean square error\"\"\"\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"rmse\")\n",
    "\n",
    "    def evaluateMAE(self, sess, dataGen):\n",
    "        \"\"\"計算 mean absolutely error\"\"\"\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定Inputs placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_inputs(self):\n",
    "    with tf.variable_scope(\"inputs\"):\n",
    "        self.isTrain = tf.placeholder(tf.bool, None)\n",
    "        # user data\n",
    "        self.query_movie_ids = tf.placeholder(tf.int32, [None, None])\n",
    "        self.query_movie_ids_len = tf.placeholder(tf.int32, [None])\n",
    "        # item data\n",
    "        self.candidate_movie_id = tf.placeholder(tf.int32, [None])\n",
    "        # labels\n",
    "        self.rating = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding部分 = One Hot Encoding + Linear Transformation<br/>\n",
    "### tensorflow 提供 tf.nn.embedding_lookup function\n",
    "1. user and item 都使用embedding\n",
    "2. **user bias, item bias 個別使用query_emb, candidate_emb projection取得, projection方式即使用tensorflow tf.matmul運算即可**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_embedding(self):\n",
    "    init_fn = tf.glorot_normal_initializer()\n",
    "    emb_init_fn = tf.glorot_uniform_initializer()\n",
    "    self.b_global = tf.Variable(emb_init_fn(shape=[]), name=\"b_global\")\n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        # embedding dictionary\n",
    "        self.w_query_movie_ids = tf.Variable(emb_init_fn(shape=[self.n_items, self.dim]), name=\"w_query_movie_ids\")\n",
    "        self.b_query_movie_ids = tf.Variable(emb_init_fn(shape=[self.dim]), name=\"b_query_movie_ids\")\n",
    "        self.w_candidate_movie_id = tf.Variable(init_fn(shape=[self.n_items, self.dim]), name=\"w_candidate_movie_id\")\n",
    "        self.b_candidate_movie_id = tf.Variable(init_fn(shape=[self.dim]), name=\"b_candidate_movie_id\")\n",
    "\n",
    "        self.query_emb = tf.nn.embedding_lookup(self.w_query_movie_ids, self.query_movie_ids)\n",
    "        self.candidate_emb = tf.nn.embedding_lookup(self.w_candidate_movie_id, self.candidate_movie_id)\n",
    "        self.candidate_bias = tf.matmul(self.candidate_emb, self.b_candidate_movie_id[:, tf.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation: 帶公式\n",
    "$$ u_i \\cdot m_j + b_u + b_m + b_{global} $$\n",
    "\n",
    "1. Sqrtn pooling: X: data, W: weight <br/>\n",
    "$ \\frac{X_1W_1 + X_2W_2 + ... + X_nW_n} {\\sqrt{W_1^2 + W_2^2 + ... W_n^2}} $ = X * normalize(W), 以這裡來說Weights全部都是1\n",
    "\n",
    "2. self.infer與self.pred節點是帶同樣的公式, 但是self.pred節點快上許多\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_computation(self):\n",
    "    with tf.variable_scope(\"computation\"):\n",
    "        '''sqrtn aggregation(pooling), X: data, W: weight\n",
    "               X_1*W_1 + X_2*W_2 + ... + X_n*W_n / sqrt(W_1**2 + W_2**2 + ... W_n**2)\n",
    "             = weighted sum of X and normalized W\n",
    "           here data = self.query_emb, weight = weighted '''\n",
    "        query_emb_mask = tf.sequence_mask(self.query_movie_ids_len)\n",
    "        weighted = tf.nn.l2_normalize(tf.to_float(query_emb_mask), 1)[:, :, tf.newaxis]\n",
    "        self.query_emb = tf.reduce_sum(self.query_emb * weighted, 1)\n",
    "        self.query_bias = tf.matmul(self.query_emb, self.b_query_movie_ids[:, tf.newaxis])\n",
    "\n",
    "        # Do: 當overfitting時, 嘗試dropout\n",
    "        # dp_rate = 0.5\n",
    "        # self.query_emb = tf.layers.dropout(self.query_emb, rate=dp_rate, training=self.isTrain)\n",
    "        # self.candidate_emb = tf.layers.dropout(self.candidate_emb, rate=dp_rate, training=self.isTrain)\n",
    "        \n",
    "        infer = tf.reduce_sum(self.query_emb * self.candidate_emb, 1, keep_dims=True)\n",
    "        infer = tf.add(infer, self.b_global)\n",
    "        infer = tf.add(infer, self.query_bias)\n",
    "        self.infer = tf.add(infer, self.candidate_bias, name=\"infer\")\n",
    "        # one query for all items\n",
    "        self.pred = tf.matmul(self.query_emb, tf.transpose(self.w_candidate_movie_id)) + \\\n",
    "                    tf.reshape(tf.matmul(self.w_candidate_movie_id, \n",
    "                                         self.b_candidate_movie_id[:, tf.newaxis]), (1, -1)) + \\\n",
    "                    self.query_bias + \\\n",
    "                    self.b_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Regularizer Term\n",
    "1. 使用MSE優化整個model\n",
    "2. RMSE and MAE loss節點並不在training graph中, 主要在eval的時候可以觀察變化\n",
    "3. 使用 L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss(self):\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # Do: 嘗試 L1 or L2 regularization term\n",
    "        self.regularizer = self.reg * (tf.reduce_sum(tf.abs(self.query_emb))     + tf.reduce_sum(tf.abs(self.query_bias)) +\n",
    "                                       tf.reduce_sum(tf.abs(self.candidate_emb)) + tf.reduce_sum(tf.abs(self.candidate_bias)))\n",
    "        # self.regularizer = self.reg * (tf.nn.l2_loss(self.query_emb)     + tf.nn.l2_loss(self.query_bias) +\n",
    "        #                                tf.nn.l2_loss(self.candidate_emb) + tf.nn.l2_loss(self.candidate_bias))\n",
    "        \n",
    "        # Do: 嘗試不同的loss function: l2_loss or mse\n",
    "        # l2_loss: Computes half the L2 norm of a tensor without the sqrt => sum(t ** 2) / 2\n",
    "        self.loss = tf.nn.l2_loss(self.infer - self.rating[:, tf.newaxis]) + self.regularizer\n",
    "        # self.loss = tf.losses.mean_squared_error(labels=self.rating[:, tf.newaxis], predictions=self.infer) + self.regularizer\n",
    "\n",
    "        # for eval\n",
    "        self.rmse_loss = tf.sqrt(tf.losses.mean_squared_error(labels=self.rating[:, tf.newaxis], predictions=self.infer))\n",
    "        self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.rating[:, tf.newaxis]))\n",
    "        pass\n",
    "    \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        # Do: 嘗試不同的Optimizer\n",
    "        # self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        # self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit: Training Function\n",
    "1. reset: if True, clean the checkpoints save data\n",
    "2. 計算epoch loss: 每個batch的loss * batch數量總和後除以epoch總數\n",
    "3. training過程中會記錄valid loss, 只會儲存最低的loss => 另一種對付overfitting的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "    \"\"\"model training\"\"\"\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if reset:\n",
    "        print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "        self.resetModel(self.modelDir)\n",
    "    self.ckpt(sess, self.modelDir)\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "    minLoss = 1e7\n",
    "    for ep in range(1, nEpoch + 1):\n",
    "        tr_loss, tr_total = 0, 0\n",
    "        for i, data in enumerate(trainGen(), 1):\n",
    "            loss, _ = sess.run([self.rmse_loss, self.train_op], feed_dict=self.feed_dict(data, mode=\"train\"))\n",
    "            tr_loss += loss ** 2 * len(data[\"query_movie_ids\"])\n",
    "            tr_total += len(data[\"query_movie_ids\"])\n",
    "            print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "        if testGen is not None:\n",
    "            epochLoss = self.epochLoss(sess, testGen)\n",
    "\n",
    "        tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "        if minLoss > epochLoss:\n",
    "            tpl += \", saving ...\"\n",
    "            self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "            minLoss = epochLoss\n",
    "\n",
    "        end = time.time()\n",
    "        print(tpl % (ep, np.sqrt(tr_loss / tr_total), epochLoss, end - start))\n",
    "        start = end\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction: 每個User產出對所有Items的評分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, sess, user_queries):\n",
    "    self.ckpt(sess, self.modelDir)\n",
    "    return sess.run(self.pred, feed_dict=self.feed_dict(user_queries, mode=\"infer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "n_batch = 128\n",
    "# Do: 嘗試不同的learning_rate [0.1, 0.001, 0.0001]\n",
    "learning_rate = 0.0001\n",
    "# Do: 嘗試不同的dim [8, 16, 20, 32]\n",
    "dim = 8\n",
    "# Do: 嘗試不同的reg係數 [0.01, 0.005, 0.0001]\n",
    "reg = 0.0001\n",
    "# 非必要: 改動model dir\n",
    "modelDir = \"./model/model_mf_with_history\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = ModelMFWithHistory(\n",
    "            n_items=nMovies,\n",
    "            dim=dim,\n",
    "            reg=reg,\n",
    "            learning_rate=learning_rate,\n",
    "            modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model.graph) as sess:\n",
    "    model.fit(sess, dataFn(trProcessed, n_batch=n_batch, shuffle=True), dataFn(teProcessed, n_batch=n_batch), nEpoch=10, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Data Function For Predict: user_data\n",
    "1. user_data function: 單純改變function參數, 可指定user id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u_data in user_data(trProcessed, [0, 1], n_batch=5):\n",
    "    break\n",
    "pd.DataFrame(u_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 單一user rating分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "u_data = list(user_data(trProcessed, [uid], n_batch=5))[0]\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred = model.predict(sess, u_data)\n",
    "print(\"shape: \", pred.shape, minmax_scale(pred.T).T)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribution\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribution\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_batch=128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    mae_ = model.evaluateMAE(sess, dataFn(teProcessed, n_batch=n_batch))\n",
    "    rmse_ = model.evaluateRMSE(sess, dataFn(teProcessed, n_batch=n_batch))\n",
    "\n",
    "print()\n",
    "print(\"MAE loss: \", mae_)\n",
    "print(\"RMSE loss: \", rmse_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 22\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    u_data = list(user_data(trProcessed, [uid], n_batch=5))[0]\n",
    "    recomm = model.predict(sess, u_data).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)), \n",
    "              \"title\": midMap[np.arange(len(recomm))].values, \n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# .query(\"rating != 0\")\n",
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "    \n",
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    predMat = []\n",
    "    for u_data in user_data(teProcessed, np.arange(nUsers), n_batch=128):\n",
    "        predMat.append(model.predict(sess, u_data))\n",
    "    predMat = np.vstack(predMat)\n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ary, neg_ary = [], []\n",
    "for label in teRatingMat:\n",
    "    label = label[label != 0]\n",
    "    pos_ary.append(sum(label >= 4))\n",
    "    neg_ary.append(sum(label < 4))\n",
    "    # print(\"pos: {}, neg: {}\".format(sum(label >= 4), sum(label < 4)))\n",
    "    \n",
    "def draw_pos_neg(idx):\n",
    "    pd.DataFrame(\n",
    "        index=idx,\n",
    "        data={\"pos\": np.array(pos_ary)[idx], \"neg\": np.array(neg_ary)[idx]}).plot.bar(figsize=(10, 5), alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "draw_pos_neg(np.arange(len(teRatingMat))[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Test Data Movie Ratings(觀察上圖)\n",
    "1. 0號, 2號, 5號, 9號 user 正向評價數量 < 10, 就算model全部預測命中, 命中率也不會是 100%!\n",
    "    ex: 0號user只有1個正向評價, 全部命中也指得到0.1的分數\n",
    "2. 3號user正向評價是負向評價的5倍多, 就算亂猜, 中的機率也很高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sum(np.sum(teRatingMat >= 4, 1) < 10)\n",
    "print(\"{} 個user正向評價總數小於10!\".format(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rating數量 >= 10 且 負評價數量 >= 正評價數量 有 [{}] 人\".format(sum(strict_condition(label) for label in teRatingMat)))\n",
    "print(\"rating正評價數量 >= 0 且 rating負評價數量 >= 0 有 [{}] 人\".format(sum(norm_condition(label) for label in teRatingMat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision At K: \n",
    "> **預測分數高(rating >= 4)的前10部電影, 和實際user rating比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat= []\n",
    "    for u_data in user_data(trProcessed, np.arange(nUsers), n_batch=n_batch):\n",
    "        pred_mat.append(model.predict(sess, u_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG: Normalized Discounted Cumulative Gain\n",
    "1. A measure of ranking quality.\n",
    "2. loop 每一位user, prediciton score排序後計算NDCG\n",
    "    <br/>$$ DCG_p = \\sum^p_{i = 1} \\frac{2^{rel_i} - 1}{log_2(i + 1)} $$<br/>\n",
    "3. IDCG: Ideal DCG, 為理想狀態下的DCG分數, 即model全部命中的DCG分數, 而NDCG: Normalized DCG, 公式如下\n",
    "    <br/>$$ NDCG_p = \\sum^p_{i = 1} \\frac{DCG_p}{IDCG_p} $$<br/>\n",
    "4. 所以NDCG是一個比值, 介於0 ~ 1之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat= []\n",
    "    for u_data in user_data(trProcessed, np.arange(nUsers), n_batch=n_batch):\n",
    "        pred_mat.append(model.predict(sess, u_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## 取出movies embedding, 使用cosine similarity列出最相似的電影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[movies.title.str.contains(\"Toy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def most_like(model, seed_movie, k=10):\n",
    "    \"\"\"給定某一部電影, 使用model裡movies embedding找尋cosine相似度高的其他電影!\"\"\"\n",
    "    with tf.Session(graph=model.graph) as sess:\n",
    "        model.ckpt(sess, model.modelDir)\n",
    "        movie_emb = sess.run(model.candidate_emb, feed_dict={model.candidate_movie_id: movies.movieId.values})\n",
    "        \n",
    "    most_like = cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb).ravel().argsort()[::-1][:k]\n",
    "    return movies.iloc[most_like]\n",
    "\n",
    "most_like(model, 0, k=10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
